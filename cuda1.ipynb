{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cuda1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPQjG7707QeieAUDT/xIz1X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NDU-CSC413/cuda1/blob/master/cuda1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction"
      ],
      "metadata": {
        "id": "j850ooJSts3j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Google colab creates a Linux instance that can be accesses from a Jupyter notebook. A Jupyter notebook consists of a sequence of cells. For our purpose there are two types of cells: **code** and **text**. For both cases you can \"execute\" the cell by pressing SHIFT-ENTER. For a text cell it will format it. You can always edit a text cell by double-clicking it.\n",
        "A code cell executes Python by default. In our case we would like to edit c++ (with CUDA extensions) files, compile and run them.\n",
        "\n",
        "First make sure that the runtime is GPU. From the menu choose Runtime->change runtime type-> Hardware accelerator choose GPU.\n",
        "\n",
        "We can run arbitrary shell commands by either preceding each one of them with a \"!\" or we can use the magic characters \"%%bash\" in the begining of the cell.\n",
        "\n",
        "As a first example let us determine the type of GPU that the instance has using the nvidia-smi command"
      ],
      "metadata": {
        "id": "U8Bcwla4txQC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "nvidia-smi"
      ],
      "metadata": {
        "id": "iUvSaO09uHMr",
        "outputId": "b23293b4-b6b1-4f03-f036-0fa1bfd7caa0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Apr  4 08:43:28 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example 0: Querying the device"
      ],
      "metadata": {
        "id": "i3S03FdSyY4H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile device.cu\n",
        "#include <iostream>\n",
        "\n",
        "\n",
        "int main()\n",
        "{\n",
        "\tint device;\n",
        "\n",
        "\tcudaDeviceProp properties;\n",
        "\tcudaError_t err = cudaSuccess;\n",
        "\terr = cudaGetDevice(&device);\n",
        "\terr = cudaGetDeviceProperties(&properties, device);\n",
        "\tstd::cout << \"processor count\" << properties.multiProcessorCount << std::endl;\n",
        "\tstd::cout << \"warp size \" << properties.warpSize << std::endl;\n",
        "\tstd::cout << \"name=\" << properties.name << std::endl;\n",
        "\tstd::cout << \"Compute capability \" << properties.major << \".\" << properties.minor << \"\\n\";\n",
        "\tstd::cout << \"shared Memory/SM \" << properties.sharedMemPerMultiprocessor\n",
        "\t\t<< std::endl;\n",
        "\n",
        "\tif (err == cudaSuccess)\n",
        "\t\tprintf(\"device =%d\\n\", device);\n",
        "\telse\n",
        "\t\tprintf(\"error getting deivce\\n\");\n",
        "\treturn 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAGYaudwyeI5",
        "outputId": "1cbc311c-2ca3-4a93-8cc6-ef7719aa764d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing device.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -o device device.cu -run"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfaVzt2VyiAS",
        "outputId": "4438a17c-8185-4f50-8aed-b195082e7874"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processor count=40\n",
            "warp size= 32\n",
            "name=Tesla T4\n",
            "Compute capability=7.5\n",
            "shared Memory/SM=65536\n",
            "device =0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFYtmiSnH5Vk"
      },
      "source": [
        "## Example1\n",
        "\n",
        "To write code, create a code cell and write %%writefile filename.cu at the beginning. To actually create/modify the file \"run\" the cell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRvUzFHU3iL5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee6dc5da-d885-4520-8d41-5904a189b2dd"
      },
      "source": [
        "%%writefile example1.cu\n",
        "#include <iostream>\n",
        "__global__ void kernel(){\n",
        "  printf(\"Hello from GPU\\n\");\n",
        "}\n",
        "int main(){\n",
        "    kernel<<<1,2>>>();\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    \n",
        "}"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing example1.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point we can open the file from the left pane. Click on the folder icon in the left pane to see all the files.\n",
        "Alternatively, we can list them using the unix \"ls\" command.\n",
        "\n",
        "**IMPORTANT**: any file created from a Jupyter notebook will be lost once the instance is restarted.\n",
        "\n",
        "A colab instance is a actually a VM running linux so each time it starts \"from scratch\". If you want to save files, mount your Google drive from the left pane."
      ],
      "metadata": {
        "id": "BvOPzAaXv0Qh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -la"
      ],
      "metadata": {
        "id": "meIQb5tvwTNd",
        "outputId": "3ed0fd32-3f78-456b-f409-ef3183d03b15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 20\n",
            "drwxr-xr-x 1 root root 4096 Apr  4 08:44 .\n",
            "drwxr-xr-x 1 root root 4096 Apr  4 08:37 ..\n",
            "drwxr-xr-x 4 root root 4096 Mar 23 14:21 .config\n",
            "-rw-r--r-- 1 root root  111 Apr  4 08:44 example1.cu\n",
            "drwxr-xr-x 1 root root 4096 Mar 23 14:22 sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcYSXgHiIQEW"
      },
      "source": [
        "NVIDIA nvcc compiler. The file extension MUST be .cu, otherwise it compiles it with a \"regular\" c++ compiler."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYIJIr9oGu0W",
        "outputId": "7d9a9dd1-8736-4282-89ca-369c90192059"
      },
      "source": [
        "!nvcc example1.cu -o example1 \n",
        "!./example1"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello from GPU\n",
            "Hello from GPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VwGQwEtIcRn"
      },
      "source": [
        "## Example2\n",
        "\n",
        "Most of the examples in the workshop use **managed memory**. In the following examples we will manage the memory manually. We will revisit managed memory later.\n",
        "\n",
        "Before computing on the GPU we need to transfer the data from host memory to device memory. Once the computation is done we transfer it back to the host.\n",
        "Below is a simple example of that process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Ii6aU0mIb4A",
        "outputId": "33599747-48fb-4c6b-9d05-7c3aa5e9126b"
      },
      "source": [
        "%%writefile example2.cu\n",
        "#include <iostream>\n",
        "__global__ void kernel(int *x,int *y,int *z){\n",
        "    *z=*x+*y;\n",
        "}\n",
        "int main(){\n",
        "    int a=1,b=2,c=0; //host variables\n",
        "    int *d_a,*d_b,*d_c;//will hold device addresses\n",
        "    // allocate memory for one integer and store the\n",
        "     // address in d_a \n",
        "    cudaMalloc(&d_a,sizeof(int));\n",
        "    cudaMalloc(&d_b,sizeof(int));\n",
        "    cudaMalloc(&d_c,sizeof(int));\n",
        "    // copy the value of a and b\n",
        "    // TO device FROM host\n",
        "    cudaMemcpy(d_a,&a,sizeof(int),cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_b,&b,sizeof(int),cudaMemcpyHostToDevice);\n",
        "    kernel<<<1,1>>>(d_a,d_b,d_c);\n",
        "    // copy the result TO host FROM device\n",
        "    cudaMemcpy(&c,d_c,sizeof(int),cudaMemcpyDeviceToHost);\n",
        "    cudaDeviceSynchronize();\n",
        "    \n",
        "std::cout<<\"value of c is \"<<c<<\"\\n\";\n",
        "    \n",
        "}"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing example2.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGlwAN9GHs4h",
        "outputId": "bc901a8c-482c-4dfb-e45d-16808dc671ed"
      },
      "source": [
        "!nvcc example2.cu -o example2\n",
        "!./example2"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "value of c is 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# More details\n",
        "Below is a similar code but with more details. In particular, it describes what and what cannot be accessed from the device and the explains the \\_\\_device\\_\\_ and \\_\\_global\\_\\_ modifiers."
      ],
      "metadata": {
        "id": "LAIwF6S200T6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile example3.cu\n",
        "#include <iostream>\n",
        "\n",
        "/**\n",
        " * @brief constant global variables are accessible from both the CPU and GPU.\n",
        " * if the constant modified is removed from the declaration of g the compiler \n",
        " * will give an error.\n",
        " * NOTE: global variables in the sense of C++ are DIFFERENT than the __global__ \n",
        " * modifier.\n",
        " * NOTE: GPU access of const global variables other than int in Windows gives an error\n",
        " * on Linux it works. \n",
        " */\n",
        "const int g=10;\n",
        "#ifdef WINDOWS\n",
        "__device__ \n",
        "#endif \n",
        "const float a=2.3;\n",
        "\n",
        "/* __device__ means a function called from the device\n",
        "* AND runs on the device\n",
        "*/\n",
        "\n",
        "__device__ float useless(){\n",
        "    return 2*a;\n",
        "}\n",
        "\n",
        "/* __global__ means a function called from the host \n",
        " * and runs on device\n",
        " */\n",
        "__global__ void example1(int *address){\n",
        "    *address=useless()*g*17;\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "int main(){\n",
        "    int x;\n",
        "    /* variable that holds device address */\n",
        "    int * device_address;\n",
        "    /* allocate space for one int on the device \n",
        "     * and store the allocated address in device_address\n",
        "     * NOTE the passing of the address of device_address variable\n",
        "     */\n",
        "    cudaMalloc(&device_address,sizeof(int));\n",
        "    /* set the value at device address to 17 using a kernel launch*/\n",
        "    std::cout<<\"address of x=\"<<std::hex<<&x<<\"\\n\";\n",
        "    std::cout<<\"device address=\"<<device_address<<\"\\n\";\n",
        "    /* launch function example1 with one block containing 1 thread \n",
        "     * @NOTE: all kernel launches are asynchronous\n",
        "     * they return immediately to host code\n",
        "     **/\n",
        "    example1 <<<1,1>>>(device_address);\n",
        "    /* copy the result back from device to host */\n",
        "    cudaMemcpy(&x,device_address,sizeof(int),cudaMemcpyDeviceToHost);\n",
        "    cudaFree(device_address);\n",
        "    std::cout<<\"The value of x=\"<<std::dec<<x<<\"\\n\";\n",
        "\n",
        "}"
      ],
      "metadata": {
        "id": "wqF5aCcS0ZTk",
        "outputId": "977bb825-3518-489b-8306-cd96a4883240",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing example3.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHdfdpmriw_8"
      },
      "source": [
        "## Example4\n",
        "This is the first example where we use parallelism, computing the sum of two arrays.\n",
        "The computation is performed where each thread computes the sum of two elements. To accomplish that we map the thread id to the array index. In this example we use a __single__, __linear__, block therefore the thread id is equal to the builtin variable threadIdx.x"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DR4-SAOvOdHq",
        "outputId": "6f614596-b76a-4739-c82e-d6828ce391be"
      },
      "source": [
        "%%writefile example4.cu\n",
        "/**\n",
        " * @file example2.cu\n",
        " * @author Hikmat Farhat (hfarhat@ndu.edu.lb)\n",
        " * @brief SAXPY example in CUDA\n",
        " * given vectors X and Y and a constant a compute a*X+Y\n",
        " * @version 0.1\n",
        " * @date 2021-12-20\n",
        " * \n",
        " * @copyright Copyright (c) 2021\n",
        " * \n",
        " */\n",
        "#include <iostream>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "\n",
        "__global__ void saxpy(float *z,float *x,float *y,float a,int n){\n",
        "   \n",
        "     int i=blockDim.x*blockIdx.x+threadIdx.x;\n",
        "     if( i<n)\n",
        "        z[i]=a*x[i]+y[i];\n",
        "\n",
        "}\n",
        "int main(){\n",
        "    /* number of elements */\n",
        "    const int n=1<<20;\n",
        "    const float a=3.0;\n",
        "/* create 3 arrays on host */\n",
        "    float *x, *y, *z;\n",
        "    x=(float *)malloc(n*sizeof(float));\n",
        "    y=(float *)malloc(n*sizeof(float));\n",
        "    z=(float *)malloc(n*sizeof(float));\n",
        "/* populate x and y */\n",
        "    for(int i=0;i<n;++i){\n",
        "        x[i]=2;\n",
        "        y[i]=4;\n",
        "    }\n",
        "/* create 3 arrays on device */\n",
        "    float *dx,*dy,*dz;\n",
        "    cudaMalloc(&dx,n*sizeof(float));\n",
        "    cudaMalloc(&dy,n*sizeof(float));\n",
        "    cudaMalloc(&dz,n*sizeof(float));\n",
        "/* transfer the values of x,y to device */\n",
        "    cudaMemcpy(dx,x,n*sizeof(float),cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(dy,y,n*sizeof(float),cudaMemcpyHostToDevice);\n",
        "    /** 1-d grid \n",
        "     * in general type dim3 has 3 values (x,y,z)\n",
        "     * when omitted each d defaults to 1\n",
        "     * e.g. dim3 a(256)=a(256,1,1)\n",
        "     * dim3 b(256,128)=b(256,128,1)\n",
        "     ***/\n",
        "    dim3 block (256);\n",
        "    dim3 grid ((n+block.x-1)/block.x,1);\n",
        "    /**\n",
        "     * NOTE:  kernel launch is asynchronous with respect\n",
        "     * to host code whereas cudaMemcpy is blocking.\n",
        "     */\n",
        "    saxpy<<<grid,block>>>(dz,dx,dy,a,n); \n",
        "    saxpy<<<grid,block>>>(dz,dx,dy,a,n); \n",
        "\n",
        "    cudaError_t e=cudaGetLastError();\n",
        "    if(e!=cudaSuccess){\n",
        "        std::cout<<cudaGetErrorString(e)<<\"\\n\";\n",
        "    }\n",
        "/* transfer the result back to host */\n",
        "    cudaMemcpy(z,dz,n*sizeof(float),cudaMemcpyDeviceToHost);\n",
        "/* check if the result is correct. We expect all values \n",
        " * of z=10\n",
        " */\n",
        "    int sum=0;\n",
        "    for(int i=0;i<n;++i)\n",
        "        sum+=z[i];\n",
        "    if (sum!=n*10)std::cout<<\"sum error\"<<sum<<\"\\n\";\n",
        "    else\n",
        "        std::cout<<\"check passed. Sum= \"<<sum<<\"\\n\";\n",
        "    free(x);\n",
        "    free(y);\n",
        "    free(z);\n",
        "    cudaFree(dx);\n",
        "    cudaFree(dy);\n",
        "    cudaFree(dz);\n",
        "}"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting example4.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lsn9ZgChkn7t",
        "outputId": "7a2d57c1-8f7d-466e-8fbb-f8b976ee2ace"
      },
      "source": [
        "!nvcc example4.cu -o example4 -run"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "check passed. Sum= 10485760\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rki06JEJliGs"
      },
      "source": [
        "## Thread blocks\n",
        "\n",
        "In CUDA the __maximum__ number of threads in a block is 1024. What if in the previous example we would like to compute the sum of two vectors with size bigger than 1024? We use multiple blocks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0t-WqnXdmMti"
      },
      "source": [
        "### Matrix addition\n",
        "We repeat the previous example by using multiple blocks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPHmBoEkmL1w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b15fb962-7650-4785-f834-8ab745371bd6"
      },
      "source": [
        "%%writefile example5.cu\n",
        "/**\n",
        " * @file example5.cu\n",
        " * @author Hikmat Farhat (hfarhat@ndu.edu.lb)\n",
        " * @brief Simple matrix addition\n",
        " * @version 0.1\n",
        " * @date 2021-12-30\n",
        " * \n",
        " * @copyright Copyright (c) 2021\n",
        " * \n",
        " */\n",
        "#include <iostream>\n",
        "const size_t width=1024;\n",
        "const size_t height=1024;\n",
        "\n",
        "//typedef float arr_t[width];\n",
        "using arr_t = float[width];\n",
        "__global__ void matrix_add(arr_t *c,arr_t *a,arr_t *b,size_t width,size_t height){\n",
        "\n",
        "    size_t idx=blockIdx.x*blockDim.x+threadIdx.x;\n",
        "    size_t idy=blockIdx.y*blockDim.y+threadIdx.y;\n",
        "    if( idx<width && idy <height)\n",
        "        //c[idx][idy]=a[idx][idy]+b[idx][idy];\n",
        "        c[idy][idx]=a[idy][idx]+b[idy][idx];\n",
        "}\n",
        "int main(){\n",
        "    arr_t *a,*b,*c;\n",
        "    cudaMallocManaged(&a,height*sizeof(arr_t));\n",
        "    cudaMallocManaged(&b,height*sizeof(arr_t));\n",
        "    cudaMallocManaged(&c,height*sizeof(arr_t));\n",
        "    for(size_t i=0;i<width;++i){\n",
        "        for(size_t j=0;j<height;++j){\n",
        "            a[i][j]=1;\n",
        "            b[i][j]=2;\n",
        "        }\n",
        "    }\n",
        "    dim3 block(32,32);\n",
        "    dim3 grid(width/block.x+1,  height/block.y+1);    \n",
        "    matrix_add<<<grid,block>>>(c,a,b,width,height);\n",
        "    cudaDeviceSynchronize();\n",
        "    bool e=false;\n",
        "    for(size_t i=0;i<width;++i)\n",
        "        for(size_t j=0;j<height;++j)\n",
        "            if(c[i][j]!=3){\n",
        "                e=true;\n",
        "                break;\n",
        "            }\n",
        "    if (e==true)std::cout<<\"error\\n\";\n",
        "}"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing example5.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "sRTxKCPn4aq5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
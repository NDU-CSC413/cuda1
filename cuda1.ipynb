{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cuda1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN5vbq0iOnSjdmtJC6UEtBI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NDU-CSC413/cuda1/blob/master/cuda1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction"
      ],
      "metadata": {
        "id": "j850ooJSts3j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Google colab creates a Linux instance that can be accesses from a Jupyter notebook. A Jupyter notebook consists of a sequence of cells. For our purpose there are two types of cells: **code** and **text**. For both cases you can \"execute\" the cell by pressing SHIFT-ENTER. For a text cell it will format it. You can always edit a text cell by double-clicking it.\n",
        "A code cell executes Python by default. In our case we would like to edit c++ (with CUDA extensions) files, compile and run them.\n",
        "\n",
        "First make sure that the runtime is GPU. From the menu choose Runtime->change runtime type-> Hardware accelerator choose GPU.\n",
        "\n",
        "We can run arbitrary shell commands by either preceding each one of them with a \"!\" or we can use the magic characters \"%%bash\" in the begining of the cell.\n",
        "\n",
        "As a first example let us determine the type of GPU that the instance has using the nvidia-smi command"
      ],
      "metadata": {
        "id": "U8Bcwla4txQC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "nvidia-smi"
      ],
      "metadata": {
        "id": "iUvSaO09uHMr",
        "outputId": "b23293b4-b6b1-4f03-f036-0fa1bfd7caa0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Apr  4 08:43:28 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example 0: Querying the device"
      ],
      "metadata": {
        "id": "i3S03FdSyY4H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile device.cu\n",
        "#include <iostream>\n",
        "\n",
        "\n",
        "int main()\n",
        "{\n",
        "\tint device;\n",
        "\n",
        "\tcudaDeviceProp properties;\n",
        "\tcudaError_t err = cudaSuccess;\n",
        "\terr = cudaGetDevice(&device);\n",
        "\terr = cudaGetDeviceProperties(&properties, device);\n",
        "\tstd::cout << \"processor count\" << properties.multiProcessorCount << std::endl;\n",
        "\tstd::cout << \"warp size \" << properties.warpSize << std::endl;\n",
        "\tstd::cout << \"name=\" << properties.name << std::endl;\n",
        "\tstd::cout << \"Compute capability \" << properties.major << \".\" << properties.minor << \"\\n\";\n",
        "\tstd::cout << \"shared Memory/SM \" << properties.sharedMemPerMultiprocessor\n",
        "\t\t<< std::endl;\n",
        "\n",
        "\tif (err == cudaSuccess)\n",
        "\t\tprintf(\"device =%d\\n\", device);\n",
        "\telse\n",
        "\t\tprintf(\"error getting deivce\\n\");\n",
        "\treturn 0;\n",
        "}"
      ],
      "metadata": {
        "id": "SAGYaudwyeI5",
        "outputId": "1cbc311c-2ca3-4a93-8cc6-ef7719aa764d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing device.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -o device device.cu -run"
      ],
      "metadata": {
        "id": "lfaVzt2VyiAS",
        "outputId": "4438a17c-8185-4f50-8aed-b195082e7874",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processor count=40\n",
            "warp size= 32\n",
            "name=Tesla T4\n",
            "Compute capability=7.5\n",
            "shared Memory/SM=65536\n",
            "device =0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFYtmiSnH5Vk"
      },
      "source": [
        "## Example1\n",
        "\n",
        "To write code, create a code cell and write %%writefile filename.cu at the beginning. To actually create/modify the file \"run\" the cell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRvUzFHU3iL5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee6dc5da-d885-4520-8d41-5904a189b2dd"
      },
      "source": [
        "%%writefile example1.cu\n",
        "#include <iostream>\n",
        "__global__ void kernel(){\n",
        "  printf(\"Hello from GPU\\n\");\n",
        "}\n",
        "int main(){\n",
        "    kernel<<<1,2>>>();\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    \n",
        "}"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing example1.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point we can open the file from the left pane. Click on the folder icon in the left pane to see all the files.\n",
        "Alternatively, we can list them using the unix \"ls\" command.\n",
        "\n",
        "**IMPORTANT**: any file created from a Jupyter notebook will be lost once the instance is restarted.\n",
        "\n",
        "A colab instance is a actually a VM running linux so each time it starts \"from scratch\". If you want to save files, mount your Google drive from the left pane."
      ],
      "metadata": {
        "id": "BvOPzAaXv0Qh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -la"
      ],
      "metadata": {
        "id": "meIQb5tvwTNd",
        "outputId": "3ed0fd32-3f78-456b-f409-ef3183d03b15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 20\n",
            "drwxr-xr-x 1 root root 4096 Apr  4 08:44 .\n",
            "drwxr-xr-x 1 root root 4096 Apr  4 08:37 ..\n",
            "drwxr-xr-x 4 root root 4096 Mar 23 14:21 .config\n",
            "-rw-r--r-- 1 root root  111 Apr  4 08:44 example1.cu\n",
            "drwxr-xr-x 1 root root 4096 Mar 23 14:22 sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcYSXgHiIQEW"
      },
      "source": [
        "NVIDIA nvcc compiler. The file extension MUST be .cu, otherwise it compiles it with a \"regular\" c++ compiler."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYIJIr9oGu0W",
        "outputId": "7d9a9dd1-8736-4282-89ca-369c90192059"
      },
      "source": [
        "!nvcc example1.cu -o example1 \n",
        "!./example1"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello from GPU\n",
            "Hello from GPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VwGQwEtIcRn"
      },
      "source": [
        "## Example2\n",
        "\n",
        "Most of the examples in the workshop use **managed memory**. In the following examples we will manage the memory manually. We will revisit managed memory later.\n",
        "\n",
        "Before computing on the GPU we need to transfer the data from host memory to device memory. Once the computation is done we transfer it back to the host.\n",
        "Below is a simple example of that process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Ii6aU0mIb4A",
        "outputId": "33599747-48fb-4c6b-9d05-7c3aa5e9126b"
      },
      "source": [
        "%%writefile example2.cu\n",
        "#include <iostream>\n",
        "__global__ void kernel(int *x,int *y,int *z){\n",
        "    *z=*x+*y;\n",
        "}\n",
        "int main(){\n",
        "    int a=1,b=2,c=0; //host variables\n",
        "    int *d_a,*d_b,*d_c;//will hold device addresses\n",
        "    // allocate memory for one integer and store the\n",
        "     // address in d_a \n",
        "    cudaMalloc(&d_a,sizeof(int));\n",
        "    cudaMalloc(&d_b,sizeof(int));\n",
        "    cudaMalloc(&d_c,sizeof(int));\n",
        "    // copy the value of a and b\n",
        "    // TO device FROM host\n",
        "    cudaMemcpy(d_a,&a,sizeof(int),cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_b,&b,sizeof(int),cudaMemcpyHostToDevice);\n",
        "    kernel<<<1,1>>>(d_a,d_b,d_c);\n",
        "    // copy the result TO host FROM device\n",
        "    cudaMemcpy(&c,d_c,sizeof(int),cudaMemcpyDeviceToHost);\n",
        "    cudaDeviceSynchronize();\n",
        "    \n",
        "std::cout<<\"value of c is \"<<c<<\"\\n\";\n",
        "    \n",
        "}"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing example2.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGlwAN9GHs4h",
        "outputId": "bc901a8c-482c-4dfb-e45d-16808dc671ed"
      },
      "source": [
        "!nvcc example2.cu -o example2\n",
        "!./example2"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "value of c is 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHdfdpmriw_8"
      },
      "source": [
        "## Example3\n",
        "This is the first example where we use parallelism, computing the sum of two arrays.\n",
        "The computation is performed where each thread computes the sum of two elements. To accomplish that we map the thread id to the array index. In this example we use a __single__, __linear__, block therefore the thread id is equal to the builtin variable threadIdx.x"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DR4-SAOvOdHq",
        "outputId": "3f31da9f-5866-4e64-fe85-9eb40ff2f01c"
      },
      "source": [
        "%%writefile example3.cu\n",
        "#include <iostream>\n",
        "\n",
        "__global__ void kernel(float* a, float* b, float* c) {\n",
        "\tint id = threadIdx.x;\n",
        "\tc[id] = a[id] + b[id];\n",
        "}\n",
        "\n",
        "int main() {\n",
        "\tint N = 1024;\n",
        "\tfloat* a, * b, * c;\n",
        "\tfloat* da, * db, * dc;\n",
        "  /* allocate memory on host */\n",
        "\ta = (float*)malloc(N * sizeof(float));\n",
        "\tb = (float*)malloc(N * sizeof(float));\n",
        "\tc = (float*)malloc(N * sizeof(float));\n",
        "  /* allocate memory on device */\n",
        "\tcudaMalloc(&da, N * sizeof(float));\n",
        "\tcudaMalloc(&db, N * sizeof(float));\n",
        "\tcudaMalloc(&dc, N * sizeof(float));\n",
        "  /* initialize the arrays a and b */\n",
        "\tfor (int i = 0; i < N; ++i) {\n",
        "\t\ta[i] = i;\n",
        "\t\tb[i] = 2 * i;\n",
        "\t}\n",
        "  /* copy arrays a and b to device */\n",
        "\tcudaMemcpy(da, a, N * sizeof(float), cudaMemcpyHostToDevice);\n",
        "\tcudaMemcpy(db, b, N * sizeof(float), cudaMemcpyHostToDevice);\n",
        "/* launch kernel with one block of N threads */\n",
        "\tkernel << <1, N >> > (da, db, dc);\n",
        "  /* copy result to host */\n",
        "\tcudaMemcpy(c, dc, N * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "  /* print the first 10 elements */\n",
        "\tfor (int i = 0; i < 10; ++i)\n",
        "\t\tstd::cout << c[i] << ' ';\n",
        "\tstd::cout << std::endl;\n",
        "\t/* free memory on host and device */\n",
        "\tfree(a);\n",
        "\tfree(b);\n",
        "\tfree(c);\n",
        "\tcudaFree(db);\n",
        "\tcudaFree(dc);\n",
        "\tcudaFree(da);\n",
        "\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting example3.cu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lsn9ZgChkn7t",
        "outputId": "6ed92655-b3a1-4b1a-de2c-6e85eb24d9dc"
      },
      "source": [
        "!nvcc example3.cu -o example3\n",
        "!./example3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 3 6 9 12 15 18 21 24 27 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rki06JEJliGs"
      },
      "source": [
        "## Thread blocks\n",
        "\n",
        "In CUDA the __maximum__ number of threads in a block is 1024. What if in the previous example we would like to compute the sum of two vectors with size bigger than 1024? We use multiple blocks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0t-WqnXdmMti"
      },
      "source": [
        "### Example4\n",
        "We repeat the previous example by using multiple blocks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPHmBoEkmL1w"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}